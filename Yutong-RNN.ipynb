{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2z3oK+dob/lbDcL70MIGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yutong-Lu/CHL5230FinalProject/blob/main/Yutong-RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pai1SE7QtN_c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('Diabetes Study File 10K Dec 14 2017.xlsx')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ZiXuIte6tV7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The result is a series where the index is 'patient_nbr' and the value is the count of records.\n",
        "patients_with_2_or_more_records = (data.groupby('Patient_ID').apply(len) >= 2)\n",
        "\n",
        "# Filter the original dataset to include only those patients who have 2 or more records.\n",
        "patients_with_2_or_more_records_df = data[data['Patient_ID'].isin(patients_with_2_or_more_records[patients_with_2_or_more_records].index)]\n",
        "\n",
        "# Update the original 'diabetes_data' dataframe with the filtered dataframe.\n",
        "data = patients_with_2_or_more_records_df\n",
        "\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fmQYZhHuUrn",
        "outputId": "3e90a16b-b654-4db9-b91d-d2d68d854816"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2497, 43)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of visits for patients\n",
        "data.groupby(['Patient_ID']).size().max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4ByDHJCyBIf",
        "outputId": "baa2d1e9-d677-4fa6-ad00-40c572f15726"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace categorical values with numerical equivalents\n",
        "data['DIABETES'].replace({'Yes' : 1, 'No' : 0}, inplace=True)\n",
        "data['Sex'].replace({'Female' : 1, 'Male' : 0}, inplace=True)\n",
        "\n",
        "# Create indicator for using hypertension meds/corticosteroid\n",
        "data['Use_of_Hypertension_Medications'] = data['Hypertension_Medications'].notnull().astype('int')\n",
        "data['Use_of_Corticosteroids'] = data['Corticosteroids'].notnull().astype('int')\n",
        "\n",
        "# Create a subset with no date\n",
        "df = data[['Patient_ID', 'Age_at_Exam', 'sBP', 'BMI', 'A1c', 'TG', 'FBS', 'Total_Cholesterol', 'Depression',\n",
        "     'HTN', 'OA', 'COPD', 'Use_of_Hypertension_Medications', 'Use_of_Corticosteroids', 'Sex', 'DIABETES']]\n",
        "\n",
        "numerical_columns = ['Age_at_Exam', 'sBP', 'BMI', 'A1c', 'TG', 'FBS', 'Total_Cholesterol']"
      ],
      "metadata": {
        "id": "nMS_pmqitbse"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping by 'Patient_ID' and creating a list of dataframes, one per group\n",
        "grouped = df.groupby('Patient_ID')\n",
        "grouped_dfs = [group for _, group in grouped]\n",
        "\n",
        "# Splitting the groups into training and test sets with an 80:20 ratio\n",
        "train_groups, test_groups = train_test_split(grouped_dfs, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reassembling the training and test datasets from the groups\n",
        "train_df = pd.concat(train_groups)\n",
        "test_df = pd.concat(test_groups)\n",
        "\n",
        "# Displaying the shape of the training and test sets\n",
        "train_df_shape = train_df.shape\n",
        "test_df_shape = test_df.shape\n",
        "\n",
        "train_df_shape, test_df_shape"
      ],
      "metadata": {
        "id": "GendIlmUtuti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688fca86-675d-47c6-dc1e-e515e5b69d9e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2017, 16), (480, 16))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_df[numerical_columns] = scaler.fit_transform(train_df[numerical_columns])\n",
        "test_df[numerical_columns] = scaler.transform(test_df[numerical_columns])"
      ],
      "metadata": {
        "id": "U3tynlQAteK3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputation\n",
        "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
        "train_filled_mice = imputer.fit_transform(train_df)\n",
        "test_filled_mice = imputer.transform(test_df)"
      ],
      "metadata": {
        "id": "3e2Qms06thCj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Group by 'patient_nbr' and create sequences\n",
        "grouped = train_df.groupby('Patient_ID', sort=False)\n",
        "sequences = [group.drop(columns=['Patient_ID', 'DIABETES']).values for _, group in grouped]\n",
        "targets = [group['DIABETES'].iloc[-1] for _, group in grouped]  # Assuming all records for a patient have the same target\n",
        "\n",
        "# Padding sequences\n",
        "max_length = max(len(s) for s in sequences)\n",
        "# We cal also go with max_length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=8, padding='post', dtype='float')\n",
        "\n",
        "padded_sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1su5sTBPxEqB",
        "outputId": "1ac97c8f-54dc-46ca-aec0-e94d5f3bf00e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(879, 8, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing sequences\n",
        "\n",
        "# Group by 'patient_nbr' and create sequences\n",
        "grouped_test = test_df.groupby('Patient_ID', sort=False)\n",
        "sequences_test = [group.drop(columns=['Patient_ID', 'DIABETES']).values for _, group in grouped_test]\n",
        "targets_test = [group['DIABETES'].iloc[-1] for _, group in grouped_test]  # Assuming all records for a patient have the same target\n",
        "\n",
        "# Padding sequences for the test set\n",
        "padded_sequences_test = pad_sequences(sequences_test, maxlen=8, padding='post', dtype='float')\n",
        "\n",
        "padded_sequences_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7Y03ovexy6j",
        "outputId": "9565ded5-9e61-4120-879c-1cf1431dd921"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(220, 8, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = padded_sequences.shape[2]\n",
        "hidden_size = 5  # Number of hidden units in RNN\n",
        "num_classes = 2  # For three-class classification\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "landa = 0.001  # Regularization term (lambda)\n",
        "\n",
        "# Data preparation\n",
        "train_dataset = TensorDataset(t.tensor(padded_sequences, dtype=t.float32), t.tensor(targets, dtype=t.long))\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(t.tensor(padded_sequences_test, dtype=t.float32), t.tensor(targets_test, dtype=t.long))\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# RNN layers and additional fully connected layer\n",
        "rnn_layer1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "dropout1 = nn.Dropout(0.5)  # Add dropout between RNN layers\n",
        "# rnn_layer2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "# dropout2 = nn.Dropout(0.5)  # Add dropout between RNN layers\n",
        "fc_layer1 = nn.Linear(hidden_size, hidden_size)  # Additional fully connected layer\n",
        "fc_layer2 = nn.Linear(hidden_size, num_classes)  # Final output layer\n",
        "\n",
        "# Dropout layer\n",
        "dropout_fc = nn.Dropout(0.2)  # Add dropout between fully connected layers\n",
        "\n",
        "# Activation function\n",
        "relu = nn.ReLU()\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = Adam(list(rnn_layer1.parameters()) + list(rnn_layer2.parameters()) + list(fc_layer1.parameters()) + list(fc_layer2.parameters()), lr=learning_rate)\n",
        "optimizer = Adam(list(rnn_layer1.parameters()) + list(fc_layer1.parameters()) + list(fc_layer2.parameters()), lr=learning_rate)\n",
        "\n",
        "# Lists to store accuracies and losses\n",
        "train_accuracy_list = []\n",
        "validation_accuracy_list = []\n",
        "train_loss_list = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    if epoch % 10 == 0:\n",
        "        learning_rate *= 0.9  # Learning rate scheduling\n",
        "\n",
        "    per_epoch_loss_list = []\n",
        "    for X, Y in train_data_loader:\n",
        "        # Forward pass through layers\n",
        "        out, _ = rnn_layer1(X)\n",
        "        out = dropout1(out)  # Apply dropout between RNN layers\n",
        "        # out, _ = rnn_layer2(out)\n",
        "        # out = dropout2(out)  # Apply dropout between RNN layers\n",
        "        out = out[:, -1, :]  # Get the last output of the sequence\n",
        "        out = relu(fc_layer1(out))  # Apply activation function after first fully connected layer\n",
        "        out = dropout_fc(out)  # Apply dropout between fully connected layers\n",
        "        out = fc_layer2(out)\n",
        "\n",
        "        # Regularization\n",
        "        # l2_term = sum([(w ** 2).sum() for w in list(rnn_layer1.parameters()) + list(rnn_layer2.parameters()) + list(fc_layer1.parameters()) + list(fc_layer2.parameters())])\n",
        "        # loss = loss_fn(out, Y) + landa * l2_term\n",
        "        loss = loss_fn(out, Y)\n",
        "\n",
        "        per_epoch_loss_list.append(loss.item())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluation on training and validation data\n",
        "    rnn_layer1.eval()\n",
        "    # rnn_layer2.eval()\n",
        "    fc_layer1.eval()\n",
        "    fc_layer2.eval()\n",
        "\n",
        "    with t.no_grad():\n",
        "        # Training data\n",
        "        train_correct, train_total = 0, 0\n",
        "        for X, Y in train_data_loader:\n",
        "            out, _ = rnn_layer1(X)\n",
        "            out = dropout1(out)  # Apply dropout between RNN layers\n",
        "            # out, _ = rnn_layer2(out)\n",
        "            # out = dropout2(out)  # Apply dropout between RNN layers\n",
        "            out = out[:, -1, :]\n",
        "            out = relu(fc_layer1(out))\n",
        "            out = dropout_fc(out)  # Apply dropout between fully connected layers\n",
        "            out = fc_layer2(out)\n",
        "            _, predicted = t.max(out.data, 1)\n",
        "            train_total += Y.size(0)\n",
        "            train_correct += (predicted == Y).sum().item()\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "        # Validation data\n",
        "        validation_correct, validation_total = 0, 0\n",
        "        for X, Y in test_data_loader:\n",
        "            out, _ = rnn_layer1(X)\n",
        "            out = dropout1(out)  # Apply dropout between RNN layers\n",
        "            # out, _ = rnn_layer2(out)\n",
        "            # out = dropout2(out)  # Apply dropout between RNN layers\n",
        "            out = out[:, -1, :]\n",
        "            out = relu(fc_layer1(out))\n",
        "            out = dropout_fc(out)  # Apply dropout between fully connected layers\n",
        "            out = fc_layer2(out)\n",
        "            _, predicted = t.max(out.data, 1)\n",
        "            validation_total += Y.size(0)\n",
        "            validation_correct += (predicted == Y).sum().item()\n",
        "        validation_accuracy = 100 * validation_correct / validation_total\n",
        "\n",
        "        # Print accuracy for the current epoch\n",
        "        print(f'Epoch {epoch}/{epochs} ---> Train Accuracy: {train_accuracy}%, Validation Accuracy: {validation_accuracy}%')\n",
        "\n",
        "        # Append accuracy values to lists\n",
        "        train_accuracy_list.append(train_accuracy)\n",
        "        validation_accuracy_list.append(validation_accuracy)\n",
        "\n",
        "    # Calculate and append the average loss for the epoch\n",
        "    train_loss_list.append(sum(per_epoch_loss_list) / len(per_epoch_loss_list))\n",
        "\n",
        "    # Set the model back to train mode\n",
        "    rnn_layer1.train()\n",
        "    # rnn_layer2.train()\n",
        "    fc_layer1.train()\n",
        "    fc_layer2.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "11e0dRRczF5x",
        "outputId": "d1998ac3-8ba0-46eb-b039-a893fe1e747b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 1/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 2/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 3/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 4/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 5/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 6/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 7/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 8/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 9/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 10/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 11/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 12/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 13/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 14/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n",
            "Epoch 15/100 ---> Train Accuracy: 30.37542662116041%, Validation Accuracy: 28.636363636363637%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-95cc7e46308b>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Evaluation on training and validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 state_steps)\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    312\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}